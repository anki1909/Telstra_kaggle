from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingClassifier
from sklearn import svm
from sklearn.linear_model import LinearRegression

from sklearn import decomposition
from collections import defaultdict
import numpy as np
import pandas as pd
from sklearn.cross_validation import StratifiedKFold
from sklearn import ensemble, preprocessing, grid_search, cross_validation
from sklearn import metrics 
from sklearn.calibration import CalibratedClassifierCV
import xgboost as xgb

import scipy.stats as scs




from sklearn.svm import SVC
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer


"""
['id', 'location', 'fault_severity', 'severity_type', 'e_counts',
       'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11',
       'l_counts', 'l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9',
       'l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18',
       'l19', 'l20', 'volume_mean', 'volume_max', 'volume_min', 'r_counts',
       'r1', 'r2', 'r3', 'r4', 'r5']


"""

def multiclass_log_loss(y_true, y_pred, eps=1e-15):
    """Multi class version of Logarithmic Loss metric.
    https://www.kaggle.com/wiki/MultiClassLogLoss

    idea from this post:
    http://www.kaggle.com/c/emc-data-science/forums/t/2149/is-anyone-noticing-difference-betwen-validation-and-leaderboard-error/12209#post12209

    Parameters
    ----------
    y_true : array, shape = [n_samples]
    y_pred : array, shape = [n_samples, n_classes]

    Returns
    -------
    loss : float
    """
    predictions = np.clip(y_pred, eps, 1 - eps)
    #print predictions
    # normalize row sums to 1
    predictions /= predictions.sum(axis=1)[:, np.newaxis]
    
    #print predictions
    actual = np.zeros(y_pred.shape)
    #print actual
    rows = actual.shape[0]
    #print rows
    actual[np.arange(rows), y_true.astype(int)] = 1
    #print 'l'
    vsota = np.sum(actual * np.log(predictions))
    return -(1.0 / rows) * vsota

def count(train,test,y):
    #print train.columns,test.columns
    train = train.replace(np.nan,0)
    test = test.replace(np.nan,0)
    #print train.groupby(y).agg('count')['fault_severity']
    count1 =  train.groupby(y).agg('count')['fault_severity']
    sum1   =  train.groupby(y).agg('sum')['fault_severity']
    #print len(count1),len(sum1),count1.index
    data_frame_count = pd.DataFrame({y:count1.index.values,'count' + str(y):count1.values})
    data_frame_sum = pd.DataFrame({y:count1.index.values,'sum'+str(y):sum1.values})
    return (data_frame_count,data_frame_sum)



def weighted_sum(train):
    return e_0[train[0]] + e_0[train[1]] + e_0[train[2]] + e_0[train[3]] +e_0[train[4]]  +e_0[train[5]] + e_0[train[6]] + e_0[train[7]] + e_0[train[8]] + e_0[train[9]] +e_0[train[10]] 
 
def weighted_sum_l1(train):
   #print len(train),l_1[train[0]],l_1[train[1]]
    return l_1[train[0]] + l_1[train[1]] + l_1[train[2]] + l_1[train[3]] +l_1[train[4]]  +l_1[train[5]] + l_1[train[6]] + l_1[train[7]] + l_1[train[8]] + l_1[train[9]] +l_1[train[10]] +l_1[train[11]] + l_1[train[12]] + l_1[train[13]] + l_1[train[14]] + l_1[train[15]] +l_1[train[16]]  +l_1[train[17]] + l_1[train[18]] + l_1[train[19]] 

def weighted_sum_l2(train):
    return l_2[train[0]] + l_2[train[1]] + l_2[train[2]] + l_2[train[3]] +l_2[train[4]]  +l_2[train[5]] + l_2[train[6]] + l_2[train[7]] + l_2[train[8]] + l_2[train[9]] +l_2[train[10]] +l_2[train[11]] + l_2[train[12]] + l_2[train[13]] + l_2[train[14]] + l_2[train[15]] +l_2[train[16]]  +l_2[train[17]] + l_2[train[18]] + l_2[train[19]] 
   
def weighted_sum_l3(train): 
    return l_3[train[0]] + l_3[train[1]] + l_3[train[2]] + l_3[train[3]] +l_3[train[4]]  +l_3[train[5]] + l_3[train[6]] + l_3[train[7]] + l_3[train[8]] + l_3[train[9]] +l_3[train[10]] +l_3[train[11]] + l_3[train[12]] + l_3[train[13]] + l_3[train[14]] + l_3[train[15]] +l_3[train[16]]  +l_3[train[17]] + l_3[train[18]] + l_3[train[19]] 
   
      

def zero_one_encoding(x):
    a = np.zeros(11)
    dct = {'resource_type1':1, 'resource_type10':10, 'resource_type2':2 ,
       'resource_type3':3, 'resource_type4':4, 'resource_type5':5,
       'resource_type6':6, 'resource_type7':7, 'resource_type8':8,
       'resource_type9':9, 'z':0 }

    a[dct[x[0]]] = a[dct[x[0]]] + 1
    a[dct[x[1]]] = a[dct[x[1]]] + 1
    a[dct[x[2]]] = a[dct[x[2]]] + 1
    a[dct[x[3]]] = a[dct[x[3]]] + 1
    a[dct[x[4]]] = a[dct[x[4]]] + 1
    #print a[0]
    return pd.Series({'R0':a[0],'R1':a[1],'R2':a[2],'R3':a[3],'R4':a[4],'R5':a[5],'R6':a[6],'R7':a[7],'R8':a[8],'R9':a[9],'R10':a[10]})
    
    
def tf_idf_cal(train,test,y):
    traindata = list(train.apply(lambda x:'%s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s' % (x['l1'],x['l2'],x['l3'],x['l4'],x['l5'],x['l6'],x['l7'],x['l8'],x['l9'],x['l10'],x['l11'],x['l12'],x['l13'],x['l14'],x['l15'],x['l16'],x['l17'],x['l18'],x['l19'],x['l20']),axis=1))
    testdata = list(test.apply(lambda x:'%s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s %s' % (x['l1'],x['l2'],x['l3'],x['l4'],x['l5'],x['l6'],x['l7'],x['l8'],x['l9'],x['l10'],x['l11'],x['l12'],x['l13'],x['l14'],x['l15'],x['l16'],x['l17'],x['l18'],x['l19'],x['l20']),axis=1))
    # the infamous tfidf vectorizer (Do you remember this one?)
    tfv = TfidfVectorizer(min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 8), use_idf=False,smooth_idf=False,sublinear_tf=1,
            stop_words = ['z'])
            
    
    # Fit TFIDF
    tfv.fit(list(traindata)+list(testdata))
    X =  tfv.transform(traindata) 
    X_test1 = tfv.transform(testdata)
    
    return X,X_test1,tfv    
    





if __name__ == '__main__':
    
    train = pd.read_csv('../input/new_train.csv')#,nrows = 300)
    test = pd.read_csv('../input/new_test.csv')
    y = train.fault_severity.copy()
    
    #loc_flt_count = train.loc[train['fault_severity'] == 0, ['location','fault_severity']].groupby('location').agg('count').reset_index()
    
    t1, t2,tfv = tf_idf_cal(train[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']] , test[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']] , y)
    t1 = pd.DataFrame(t1.toarray(), columns = tfv.get_feature_names())
    t2 = pd.DataFrame(t2.toarray(), columns = tfv.get_feature_names())
    t1[t1 !=0] = 1
    t2[t2 !=0] = 1
    
    
    
    
    
    
    
    """
    >>> len(np.unique(loc_flt_count.location))
    808
    >>> 929 - 808
    121
    >>> len([i for i in np.unique(test.location) if i not in np.unique(train.location)])
    197
    >>> len([i for i in np.unique(test.location) if i not in np.unique(loc_flt_count.location)])
    302
    >>> 302 - 197
    105
    >>> 
    
    """
    
    location_count = pd.read_csv('../input/location_count.csv')
    train = train.merge(location_count , on = 'location' , how = 'left')
    test = test.merge(location_count , on = 'location' , how = 'left')
    

    
    e_0 = pd.read_csv('../input/fault_0_count.csv')
    e_1 = pd.read_csv('../input/fault_1_count.csv')
    e_2 = pd.read_csv('../input/fault_2_count.csv') 
    e_0.rename(columns={'event_type':'e1'}, inplace=True)
    e_0.rename(columns = {'fault_severity':'e1_counts'},inplace= True)
    e_1.rename(columns={'event_type':'e2'}, inplace=True)
    e_1.rename(columns = {'fault_severity':'e1_counts'},inplace= True)
    
    
    
    
    train = train.merge(e_1, on = 'e2', how = 'left')
    test = test.merge(e_1, on = 'e2', how = 'left')
    
    
    
    e_0 =  e_0.set_index('e1')['e1_counts'].to_dict()
    e_0 = defaultdict(int, e_0)
    train['weighted_sum_e1'] = train[['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11']].apply(weighted_sum, axis=1)
    test['weighted_sum_e1'] = test[['e1','e2','e3','e4','e5','e6','e7','e8','e9','e10','e11']].apply(weighted_sum, axis=1)
   
   
    
    l_00 = pd.read_csv('../input/fault_0_log_count.csv')
    l_10 = pd.read_csv('../input/fault_1_log_count.csv')
    l_20 = pd.read_csv('../input/fault_2_log_count.csv')
    l_1 =  l_00.set_index('log_feature')['fault_severity'].to_dict()
    l_1 = defaultdict(int, l_1)
    l_2 =  l_10.set_index('log_feature')['fault_severity'].to_dict()
    l_2 = defaultdict(int, l_2)
    l_3 =  l_20.set_index('log_feature')['fault_severity'].to_dict()
    l_3 = defaultdict(int, l_3)
    
    
    
    print 'adsasdsasda'
    train['weighted_sum_l1'] = train[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l1, axis=1)
    test['weighted_sum_l1'] = test[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l1, axis=1)
   
    train['weighted_sum_l2'] = train[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l2, axis=1)
    test['weighted_sum_l2'] = test[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l2, axis=1)
           
    train['weighted_sum_l3'] = train[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l3, axis=1)
    test['weighted_sum_l3'] = test[['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20']].apply(weighted_sum_l3, axis=1)
      
    
    
    
    
    
    
    
    
    # intraction
    
    temp = train.append(test)
    l = train[['r1','e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type']].append(test[['r1','e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type']])
    l = l[['r1','e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type']].groupby\
    (['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type']).agg('count').reset_index()
    l.rename(columns={'r1':'all_e1_counts'}, inplace=True)
    train = pd.merge(train,l,on = ['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type'] , how = 'left')
    test = pd.merge(test,l,on = ['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10', 'e11','severity_type'] , how = 'left')
    
    # logloss has been reduced little bit
    #intractions
    # intraction
    l = train[['r1','l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','severity_type']].append(test[['r1','l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','severity_type']])
    l =l.groupby\
    (['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','severity_type']).agg('count').reset_index()
    l.rename(columns={'r1':'l1_all_counts'}, inplace=True)
    train = pd.merge(train,l,on = ['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','severity_type'] , how = 'left')
    test = pd.merge(test,l,on = ['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','severity_type'] , how = 'left')
   
    
    # intraction l with location 
    
    l = train[['r1','l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','location']].append(test[['r1','l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','location']])
    l =l.groupby\
    (['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','location']).agg('count').reset_index()
    l.rename(columns={'r1':'lss1_all_counts'}, inplace=True)
    train = pd.merge(train,l,on = ['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','location'] , how = 'left')
    test = pd.merge(test,l,on = ['l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'l7', 'l8', 'l9','l10', 'l11', 'l12', 'l13', 'l14', 'l15', 'l16', 'l17', 'l18','l19', 'l20','location'] , how = 'left')
   
   
    #event _location
   
    
   
   
   
    
   
    id = test.id.values
    y = train.fault_severity.values
    
  
    
    print train.shape,test.shape
    
    
    
    
         
    
    
    
    
    k_train = ['location_counts','e1_counts','weighted_sum_l3','weighted_sum_l2','weighted_sum_l1','weighted_sum_e1','lss1_all_counts','l1_all_counts', 'all_e1_counts','location', 'severity_type', 'e_counts', 'l_counts','volume_mean', 'volume_max', 'volume_min', 'r_counts','e1','l1','r1','r2','l2','e2','e3','l3','fault_severity']
    k_test =  ['location_counts','e1_counts','weighted_sum_l3','weighted_sum_l2','weighted_sum_l1','weighted_sum_e1','lss1_all_counts','l1_all_counts','all_e1_counts','location', 'severity_type', 'e_counts', 'l_counts','volume_mean', 'volume_max', 'volume_min', 'r_counts','e1','l1','r1','r2','l2','e2','e3','l3']
    
    #['1', u'event_type1', u'event_type1 event_type11']
    
    train = train[k_train]
    test = test[k_test]
    
    #train = pd.concat((train, t1[['event_type1 event_type11 event_type15']]), axis=1)
    #test = pd.concat((test, t2[['event_type1 event_type11 event_type15']]), axis=1)
   
    print 'before',test.shape,test.shape,train.shape,train.shape
    text_columns = []
    
  
   
   
   
   
   
    text_columns = []
    for f in train.columns:
        if train[f].dtype=='object':
            print f
            text_columns.append(f)            
            lbl = preprocessing.LabelEncoder()
            lbl.fit(list(train[f].values) + list(test[f].values))
            train[f] = lbl.transform(list(train[f].values))
            test[f] = lbl.transform(list(test[f].values))
     
     
    
    temp = train[['l1','location','severity_type']].append(test[['l1','location','severity_type']])
 
    for i in np.unique(temp.location):    
        ind_1 = (temp.location == i) & (temp.severity_type == 0)
        #ind_2 = (temp.location == i) & (temp.severity_type == 1)
        #train.loc[train.location == i, 'type0'] = temp.loc[(temp.location == i) & (temp.severity_type == 3)].shape[0]/float(temp.loc[(temp.location == i)].shape[0])
        train.loc[train.location == i, 'type1'] = temp.loc[(temp.location == i) & (temp.severity_type == 1)].shape[0]/float(temp.loc[(temp.location == i)].shape[0])
        
        #test.loc[test.location == i, 'type0'] = temp.loc[(temp.location == i) & (temp.severity_type == 3)].shape[0]/float(temp.loc[(temp.location == i)].shape[0])
        test.loc[test.location == i, 'type1'] = temp.loc[(temp.location == i) & (temp.severity_type == 1)].shape[0]/float(temp.loc[(temp.location == i)].shape[0])
        
    
    
    k1 = pd.DataFrame(temp.groupby(['location','severity_type']).agg('count')).reset_index()
    k1.rename(columns={'l1':'one_way_intraction_location_severitytype'}, inplace=True)
    
    
    
    
    
    
    train = pd.merge(train,k1,on = ['location','severity_type'] , how = 'left')    
    test = pd.merge(test,k1,on = ['location','severity_type'] , how = 'left')    
    
    
    
    
    
    k = train.columns
      
    test = test.replace(np.nan, -1)
    train = train.replace(np.nan , -1)
    
    train = train.drop('fault_severity',axis = 1)
    
    n_folds = 5
    gbm = ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.01,  min_samples_split = 30,subsample =1 , max_features = 'sqrt' ,  n_estimators = 1000 , max_depth =  6)
    mean_score = 10  
    train2 = train.copy()        
    columns = []
    print train.shape
    for j in (list('1') + list(t1.columns.values)):
        skf = list(StratifiedKFold(y, n_folds))
       
        train = train2.copy()
        print train.shape[1],'p'
        score = []
        if j != '1':
            train[j] = t1[j].copy()
            print train.shape[1],'b',j,mean_score
        for i, (train1, test1) in enumerate(skf):
            
            X_train = train.iloc[train1]
            y_train = y[train1]
            X_test = train.iloc[test1]
            y_test =  y[test1]
            gbm.fit(X_train, y_train)
            test_prob = gbm.predict_proba(X_test)
            #print type(test_prob)
            comupted_score = multiclass_log_loss(y_test, test_prob)
            score.append(comupted_score)
        print 'score',np.mean(score),score
        if np.mean(score) < mean_score:
            mean_score = np.mean(score)
            train2 = train.copy()
            columns.append(j)
            
            
            
            #0.502414734595
    
    
    
    
    
  
   
    gbm = ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.01,subsample = 1,  min_samples_split = 30 , max_features = 'sqrt' ,  n_estimators = 1000 , max_depth =  6)
    
    gbm.fit(train,y)
    pred = gbm.predict_proba(test)
    
    submission = pd.DataFrame({'id':id,'predict_0':pred[:,0],'predict_1':pred[:,1],'predict_2':pred[:,2]})
    submission.to_csv('without_taget_submission.csv',index = False)
    
   
    #0.50176680771684312
   
   
